{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Описание решения\n",
    "\n",
    "Исполльзовалось два алгоритма:\n",
    "\n",
    "    1) Деревья (sklearn.ensemble.RandomForestClassifier)\n",
    "    2) Логистическую регрессию (sklearn.linear_model.LogisticRegression)\n",
    "\n",
    "И деревья и логистическая регрессия обучались на следующих признаках:\n",
    "    * количество слов в тексте вопроса (получено с помощью nltk), так же квадрат этого количества\n",
    "    * количество слов в заголовке и в тегах (получено с помощью nltk), так же квадрат этого количества\n",
    "    * количество предложений в тексте вопроса (получено с помощью nltk), так же квадрат этого количества\n",
    "    * длина текста\n",
    "    * длина заголовка и тегов\n",
    "    * момент создания вопроса\n",
    "    * момент создания аккаунта автора вопроса\n",
    "    * темпы роста репутации автора (как репутация автора / разность создания вопроса и создания аккаунта автора)\n",
    "    * репутация автора и логарифм репутации автора\n",
    "    * для некторых слов подсчитывалось сколько раз они встретились в тесте и в заголовке ('    ' - сиволизирует количество строчек кода в вопросе, '\\n' - количество абзацев...)\n",
    "    \n",
    "Логистическая регрессия(кроме вышеперечисленных) также обучалась на следующих признаках:\n",
    "    * tfidf по тексту вопроса\n",
    "    * tfidf по тексту заголовка и тегов вопроса\n",
    "    * tfidf по тексту первого абзаца вопроса\n",
    "    * посимвольный tfidf по тексту вопроса\n",
    "    * посимвольный tfidf по тексту заголовка и тегов вопроса\n",
    "\n",
    "Далее два алгоритма линейно конкатенировались."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import nltk\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "train_path = 'real-train-contest.csv'\n",
    "test_path = 'test-contest.csv'\n",
    "real_test_path = 'test-contest-second.csv'\n",
    "data = pd.read_csv(train_path)\n",
    "datatest = pd.read_csv(test_path)\n",
    "datarealtest = pd.read_csv(real_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_time(x):\n",
    "    try:\n",
    "        return time.mktime(datetime.datetime.strptime(x, '%m/%d/%Y %H:%M:%S').timetuple())\n",
    "    except:\n",
    "        return time.mktime(datetime.datetime.strptime(x, '%Y-%m-%d').timetuple())\n",
    "def make_new_features(data):   \n",
    "    ls = []\n",
    "    for ind,row in data.iterrows():        \n",
    "        s = str(row.Tag1) + ' ' + str(row.Tag2) + ' ' + str(row.Tag3) + ' ' + str(row.Tag4) + ' ' + str(row.Tag5) + ' ' + str(row.Title)\n",
    "        ls.append(s)\n",
    "    data['Tags'] = ls \n",
    "    data['CntWord'] = data.BodyMarkdown.apply(lambda x: len(nltk.word_tokenize(re.sub('\\W+', ' ', x.lower()))))\n",
    "    data['CntTitleWord'] = data.Title.apply(lambda x: len(nltk.word_tokenize(re.sub('\\W+', ' ', x.lower()))))\n",
    "    data['CntSentance'] = data.BodyMarkdown.apply(lambda x: len(nltk.sent_tokenize(x.lower())))\n",
    "    data['TextLength'] = data.BodyMarkdown.apply(lambda x: len(x.split()))\n",
    "    data['HeaderLength'] = data.Title.apply(lambda x: len(x.split()))\n",
    "    data['PostCreation'] = data.PostCreationDate.apply(lambda x: get_time(x))\n",
    "    data['OwnerCreation'] = data.OwnerCreationDate.apply(lambda x: get_time(x))\n",
    "    data['PostAge'] = data.PostCreation - data.OwnerCreation\n",
    "    data['PostAge'] = data.PostAge.apply(lambda x: max(x, 1))\n",
    "    data['FirstIndent'] = data.BodyMarkdown.apply(lambda x: x[: x.find('\\n')])\n",
    "    data['TempInc'] = data.ReputationAtPostCreation / data.PostAge\n",
    "    data['NormReputation'] = data.ReputationAtPostCreation.apply(lambda x: math.log(x + 100))\n",
    "    data['NormCntWord'] = data.CntWord * data.CntWord\n",
    "    data['NormCntTitleWord'] = data.CntTitleWord * data.CntTitleWord\n",
    "    data['NormCntSentance'] = data.CntSentance * data.CntSentance\n",
    "    \n",
    "make_new_features(data)\n",
    "make_new_features(datatest)\n",
    "make_new_features(datarealtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n"
     ]
    }
   ],
   "source": [
    "print('!!!')\n",
    "data_train = data\n",
    "data_test = datatest\n",
    "data_real_test = datarealtest\n",
    "feature_set = ['ReputationAtPostCreation', 'OwnerUndeletedAnswerCountAtPostTime', 'TextLength', 'PostCreation', \n",
    "               'OwnerCreation', 'HeaderLength', 'CntWord', 'CntTitleWord', 'CntSentance', 'TempInc', 'NormReputation', \n",
    "               'NormCntWord', 'NormCntTitleWord', 'NormCntSentance']\n",
    "X_train = np.array(data_train[feature_set])\n",
    "X_test = np.array(data_test[feature_set])\n",
    "X_real_test = np.array(data_real_test[feature_set])\n",
    "y_train = np.array(data_train.OpenStatus)\n",
    "y_test = np.array(data_test.OpenStatus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = ['my', 'your', 'favorite', 'best', 'random', 'python', 'up', 'down', 'change', 'qsort', 'pascal', ':=', 'the',\n",
    "         'and', 'to', 'have', 'look', 'this', 'from', 'in' 'for', 'if', 'else', 'import', 'def', 'void', 'int', 'double', \n",
    "         'back', 'time', 'now', 'just', 'more', 'way', 'to', 'try', 'call', 'help', 'work', 'window', 'step', 'tree', 'name', \n",
    "         'path', 'both', 'sure', 'last', 'next', '\\n', '    ', 'an', 'data', 'real', 'tree', 'host', 'unix', 'win', 'xp', \n",
    "         'phone', ]\n",
    "def add_counts_words(X, data, words):\n",
    "    for w in words:\n",
    "        y = data.Title.apply(lambda x: x.lower().count(w))\n",
    "        y = np.array(y)[:, np.newaxis]\n",
    "        X = np.hstack((X, y))\n",
    "        y = data.BodyMarkdown.apply(lambda x: x.lower().count(w))\n",
    "        y = np.array(y)[:, np.newaxis]\n",
    "        X = np.hstack((X, y))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = add_counts_words(X_train, data, words)\n",
    "X_test = add_counts_words(X_test, datatest, words)\n",
    "X_real_test = add_counts_words(X_real_test, datarealtest, words)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_real_test = scaler.transform(X_real_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Деревья"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800188757893\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=25, min_samples_leaf=11, n_estimators=170)\n",
    "clf.fit(X_train, y_train)\n",
    "print(roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "#0.775223476161\n",
    "#0.800654597744"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = list(np.concatenate((np.array(data_train.BodyMarkdown), np.array(data_real_test.BodyMarkdown))))\n",
    "tags = list(np.concatenate((np.array(data_train.Tags), np.array(data_real_test.Tags))))\n",
    "docs_2 = list(np.concatenate((np.array(data_train.FirstIndent), np.array(data_real_test.FirstIndent))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(min_df=10)#, stop_words='english')\n",
    "vect.fit(docs)\n",
    "X_train_tfidf = vect.transform(data_train.BodyMarkdown)\n",
    "X_test_tfidf = vect.transform(data_real_test.BodyMarkdown)\n",
    "print('!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n"
     ]
    }
   ],
   "source": [
    "vect_tags = TfidfVectorizer(min_df=5)#, stop_words='english')\n",
    "vect_tags.fit(tags)\n",
    "X_train_tfidf_tags = vect_tags.transform(data_train.Tags)\n",
    "X_test_tfidf_tags = vect_tags.transform(data_real_test.Tags)\n",
    "print('!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n"
     ]
    }
   ],
   "source": [
    "vect_2 = TfidfVectorizer(min_df=10)#, stop_words='english')\n",
    "vect_2.fit(docs_2)\n",
    "X_train_tfidf_2 = vect_2.transform(data_train.FirstIndent)\n",
    "X_test_tfidf_2 = vect_2.transform(data_real_test.FirstIndent)\n",
    "print('!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n"
     ]
    }
   ],
   "source": [
    "vect_chars = TfidfVectorizer(min_df=50, analyzer='char')\n",
    "vect_chars.fit(docs)\n",
    "X_train_tfidf_chars = vect_chars.transform(data_train.BodyMarkdown)\n",
    "X_test_tfidf_chars = vect_chars.transform(data_real_test.BodyMarkdown)\n",
    "print('!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n"
     ]
    }
   ],
   "source": [
    "vect_chars_tags = TfidfVectorizer(min_df=10, analyzer='char')\n",
    "vect_chars_tags.fit(tags)\n",
    "X_train_tfidf_chars_tags = vect_chars_tags.transform(data_train.Tags)\n",
    "X_test_tfidf_chars_tags = vect_chars_tags.transform(data_real_test.Tags)\n",
    "print('!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n"
     ]
    }
   ],
   "source": [
    "print('!!!')\n",
    "import scipy.sparse as sps\n",
    "X_train_2 = sps.hstack((X_train, X_train_tfidf))\n",
    "X_test_2 = sps.hstack((X_real_test, X_test_tfidf))\n",
    "\n",
    "X_train_2 = sps.hstack((X_train_2, X_train_tfidf_tags))\n",
    "X_test_2 = sps.hstack((X_test_2, X_test_tfidf_tags))\n",
    "\n",
    "X_train_2 = sps.hstack((X_train_2, X_train_tfidf_2))\n",
    "X_test_2 = sps.hstack((X_test_2, X_test_tfidf_2))\n",
    "\n",
    "X_train_2 = sps.hstack((X_train_2, X_train_tfidf_chars))\n",
    "X_test_2 = sps.hstack((X_test_2, X_test_tfidf_chars))\n",
    "\n",
    "X_train_2 = sps.hstack((X_train_2, X_train_tfidf_chars_tags))\n",
    "X_test_2 = sps.hstack((X_test_2, X_test_tfidf_chars_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Линейные модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_linear = LogisticRegression()\n",
    "clf_linear.fit(X_train_2, y_train)\n",
    "#print(roc_auc_score(y_test, clf_linear.predict_proba(X_test_2)[:, 1]))\n",
    "#0.864784895409\n",
    "#0.87036472842 without stop_words\n",
    "#0.870540386559 without stop_words + new words(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### но мы потратили время не зря!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63 0.875840143714\n",
      "0.64 0.875912756658\n",
      "0.65 0.875970910645\n",
      "0.66 0.876009822196\n",
      "0.67 0.876034352479\n",
      "0.68 0.876041777416\n",
      "0.69 0.876031825707\n",
      "0.7 0.876012171391\n",
      "0.71 0.875978170336\n",
      "0.72 0.875928081303\n",
      "0.73 0.875866665572\n"
     ]
    }
   ],
   "source": [
    "p1 = clf_linear.predict_proba(X_test_2)[:, 1]\n",
    "p2 = clf.predict_proba(X_test)[:, 1]\n",
    "for alpha in np.arange(0, 1.01, 0.01):\n",
    "    p = roc_auc_score(y_test, alpha * p1 + (1 - alpha) * p2)\n",
    "    if p > 0.8758:\n",
    "        print(alpha, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n"
     ]
    }
   ],
   "source": [
    "print('!!!')\n",
    "p1 = clf_linear.predict_proba(X_test_2)[:, 1]\n",
    "p2 = clf.predict_proba(X_real_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.60\n",
    "ans = alpha * p1 + (1 - alpha) * p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = datarealtest.copy()\n",
    "res['OpenStatus'] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res.to_csv('result.csv', mode='w', index=False, columns=['PostId', 'OpenStatus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
